{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deeafe60-f1f6-4682-8c88-0df53eba3613",
   "metadata": {},
   "source": [
    "# Load Data, Display Images, Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98db30c9-1acd-4834-aff8-bcc820f85183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0ad2f-1b47-40e2-adb7-881a144c9ef3",
   "metadata": {},
   "source": [
    "## Make Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f0193f-026b-4424-a09d-bb943d026a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "download = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342b1995-9907-4667-ac56-b1352ac4147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#create Datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                download=download)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                               download=download)\n",
    "\n",
    "#create data Loaders\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ffff5b-0863-40ac-a1a4-690665df299e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 152, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(trainloader)\n\u001b[0;32m----> 2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 152, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d6868160-30c2-4ee7-bea9-ba18dc81c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 64, 64)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load np array\n",
    "speech_labels = np.load(\"../data/labels/high_dim/cifar10_speech.npy\")\n",
    "speech_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1296e-34db-428d-b531-5a003b96cf3d",
   "metadata": {},
   "source": [
    "## Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6a5cc-c313-4286-8d15-883b908bbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d3f3b882-d202-4883-bec9-a48d61b4d92b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 152, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes)\n\u001b[1;32m      3\u001b[0m images_by_class \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes)]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n\u001b[1;32m      7\u001b[0m         images_by_class[label]\u001b[38;5;241m.\u001b[39mappend(images[i])\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 277, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 144, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ryanrodriguez/PycharmProjects/personal/standard_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 152, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "classes = ('aiplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "num_classes = len(classes)\n",
    "images_by_class = [[] for _ in range(num_classes)]\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    for i, label in enumerate(labels):\n",
    "        images_by_class[label].append(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01f666-5f38-4fd1-954e-2660d26ee01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fc371-5155-4999-8b53-1bc7924741a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, mean, std):\n",
    "    mean = mean[:, None, None]\n",
    "    std = std[:, None, None]\n",
    "    img = img.numpy()  # Convert tensor to numpy array\n",
    "    img = std * img + mean \n",
    "    img = np.transpose(img, (1, 2, 0))  # Change dimension order to HxWxC\n",
    "    img = np.clip(img, 0, 1)  # Clip values to ensure they're within [0, 1]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Don't show axes for a cleaner image\n",
    "\n",
    "for class_index, class_images in enumerate(images_by_class):\n",
    "    print(f\"Class: {classes[class_index]}\")\n",
    "    plt.figure(figsize=(3,3))\n",
    "    for i, image in enumerate(class_images[:5]):  # Display first 5 images of each class\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.axis('off')\n",
    "        imshow(image, mean, std)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f599ee-0a32-43fb-940a-722d5133ae9c",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0f22d30a-8542-483d-9a0d-226ff6989d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')\n",
    "label_map = {idx: label for idx,label in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "01b8596c-a6d6-4578-8da3-993e190483de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Airplane'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ed7d7776-8956-4edb-b541-5ffa07b572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_save_dir = \"../data/imgs/cifar10\"\n",
    "label_save_dir = \"../data/labels/categorical/\"\n",
    "\n",
    "image_file_names = [\"cifar10_train_img.npy\", \"cifar10_test_img.npy\"]\n",
    "label_file_names = [\"cifar10_training.npy\", \"cifar10_test.npy\"]\n",
    "data = [trainset, testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "65941b00-b810-4f1a-9cbc-d4bec3e3e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels: (50000,), shape of imgs (50000, 32, 32, 3)\n",
      "Shape of labels: (10000,), shape of imgs (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "for dataset, image_file_name, label_file_name in zip(data, image_file_names, label_file_names):\n",
    "    imgs, labels = zip(*dataset)\n",
    "    imgs  = np.stack([img for img in imgs])\n",
    "    labels = np.array(labels)\n",
    "    print(f\"Shape of labels: {labels.shape}, shape of imgs {imgs.shape}\")\n",
    "    np.save(os.path.join(image_save_dir, image_file_name), imgs) # save images\n",
    "    np.save(os.path.join(label_save_dir, label_file_name), labels) #save labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "411ce81d-c185-431e-b926-43ff8f476239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesWHighDimLabels(Dataset):\n",
    "    def __init__(self, img_path:str, cat_label_path:str, high_dim_labels_path:str, label_map:dict, transform=None):\n",
    "        self.images = np.load(img_path)\n",
    "        self.cat_labels = np.load(cat_label_path)\n",
    "        self.high_dim_labels = np.load(high_dim_labels_path)\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cat_labels)\n",
    "\n",
    "    def display_item(self, idx):\n",
    "        img = self.images[idx]\n",
    "        cat_label = self.cat_labels[idx]\n",
    "        label_text = self.label_map[cat_label]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(2,2))\n",
    "        ax.axis('off')\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(label_text, fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    def display_item_w_label(self, idx):\n",
    "        img = self.images[idx]\n",
    "        cat_label = self.cat_labels[idx]\n",
    "        label_text = self.label_map[cat_label]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(4,2))\n",
    "        \n",
    "        ax[0].axis('off')\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].set_title(label_text, fontsize=10)\n",
    "\n",
    "        ax[1].axis('off')\n",
    "        ax[1].imshow(self.high_dim_labels[cat_label])\n",
    "        ax[1].set_title(\"Spectrogram\", fontsize=10)\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        cat_label = self.cat_labels[idx]\n",
    "        high_dim_label = self.high_dim_labels[cat_label]        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, high_dim_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8d65bd12-d2ac-49ce-aeb8-455314b3beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.247, 0.243, 0.261]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=mean,  \n",
    "                             std=std)\n",
    "    ])\n",
    "\n",
    "img_path = os.path.join(image_save_dir, image_file_names[0])\n",
    "cat_label_path = os.path.join(label_save_dir, label_file_names[0])\n",
    "high_dim_labels_path = \"../data/labels/high_dim/cifar10_speech.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "bae91551-f050-45c6-b3c3-a3a91b5c9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "will_this_work = ImagesWHighDimLabels(img_path, cat_label_path, high_dim_labels_path, label_map, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "38d9ab52-5024-4014-891a-06d6375c3ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ImagesWHighDimLabels at 0x297487ee0>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "will_this_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "9e6aaf38-a461-433b-9f27-8350991ac6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0527e+00, -1.3068e+00, -1.1956e+00,  ...,  5.1906e-01,\n",
       "            4.2380e-01,  3.6029e-01],\n",
       "          [-1.7354e+00, -1.9895e+00, -1.7037e+00,  ..., -3.6628e-02,\n",
       "           -1.0013e-01, -5.2505e-02],\n",
       "          [-1.5926e+00, -1.7354e+00, -1.2115e+00,  ..., -1.1601e-01,\n",
       "           -8.4258e-02, -2.5890e-01],\n",
       "          ...,\n",
       "          [ 1.3129e+00,  1.2018e+00,  1.1541e+00,  ...,  5.5081e-01,\n",
       "           -1.1004e+00, -1.1480e+00],\n",
       "          [ 8.6835e-01,  7.5721e-01,  9.6361e-01,  ...,  9.3186e-01,\n",
       "           -4.4942e-01, -6.7170e-01],\n",
       "          [ 8.2072e-01,  6.7783e-01,  8.5247e-01,  ...,  1.4399e+00,\n",
       "            4.0792e-01, -3.6628e-02]],\n",
       " \n",
       "         [[-9.8380e-01, -1.2420e+00, -1.2097e+00,  ...,  1.4587e-01,\n",
       "            3.2906e-02,  1.6768e-02],\n",
       "          [-1.6616e+00, -1.9844e+00, -1.8553e+00,  ..., -5.6421e-01,\n",
       "           -6.4490e-01, -5.8034e-01],\n",
       "          [-1.5970e+00, -1.8714e+00, -1.5486e+00,  ..., -6.2876e-01,\n",
       "           -6.2876e-01, -8.0628e-01],\n",
       "          ...,\n",
       "          [ 7.5912e-01,  4.8477e-01,  6.1388e-01,  ...,  1.6201e-01,\n",
       "           -1.4841e+00, -1.4357e+00],\n",
       "          [ 2.5884e-01,  6.2941e-04,  3.3953e-01,  ...,  4.0408e-01,\n",
       "           -9.8380e-01, -1.1290e+00],\n",
       "          [ 3.3953e-01,  9.7458e-02,  3.0725e-01,  ...,  9.8506e-01,\n",
       "           -8.0061e-02, -4.9965e-01]],\n",
       " \n",
       "         [[-7.6414e-01, -1.0346e+00, -1.0646e+00,  ..., -8.8010e-02,\n",
       "           -1.7816e-01, -1.6314e-01],\n",
       "          [-1.4102e+00, -1.7107e+00, -1.7107e+00,  ..., -8.8434e-01,\n",
       "           -9.5947e-01, -8.5429e-01],\n",
       "          [-1.3952e+00, -1.7107e+00, -1.5905e+00,  ..., -9.5947e-01,\n",
       "           -9.5947e-01, -1.0797e+00],\n",
       "          ...,\n",
       "          [-2.6831e-01, -1.1999e+00, -1.3201e+00,  ..., -6.5897e-01,\n",
       "           -1.6056e+00, -1.4102e+00],\n",
       "          [-2.6831e-01, -1.0797e+00, -1.2600e+00,  ..., -2.9836e-01,\n",
       "           -1.1999e+00, -1.1999e+00],\n",
       "          [ 3.2191e-02, -2.9836e-01, -4.0354e-01,  ...,  3.9280e-01,\n",
       "           -4.4861e-01, -6.2892e-01]]]),\n",
       " array([[-71.88831 , -68.091866, -64.4265  , ..., -76.037636, -80.      ,\n",
       "         -80.      ],\n",
       "        [-70.24244 , -66.515884, -62.679657, ..., -76.82332 , -80.      ,\n",
       "         -80.      ],\n",
       "        [-66.31116 , -61.00439 , -56.150875, ..., -76.63746 , -80.      ,\n",
       "         -80.      ],\n",
       "        ...,\n",
       "        [-79.58982 , -77.19187 , -71.51244 , ..., -80.      , -80.      ,\n",
       "         -80.      ],\n",
       "        [-79.41027 , -75.96264 , -69.60703 , ..., -80.      , -80.      ,\n",
       "         -80.      ],\n",
       "        [-79.410324, -75.962975, -69.68023 , ..., -80.      , -80.      ,\n",
       "         -80.      ]], dtype=float32))"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "will_this_work[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02007f4-190c-4328-8571-a0bc0fa59fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e3296-adc5-4551-b645-bbd27a867cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
